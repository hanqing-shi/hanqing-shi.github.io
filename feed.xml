<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://hanqing-shi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hanqing-shi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-31T17:03:37+00:00</updated><id>https://hanqing-shi.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Least Squares</title><link href="https://hanqing-shi.github.io/blog/2023/linear-algebra-note-1/" rel="alternate" type="text/html" title="Least Squares"/><published>2023-03-19T15:12:00+00:00</published><updated>2023-03-19T15:12:00+00:00</updated><id>https://hanqing-shi.github.io/blog/2023/linear-algebra-note-1</id><content type="html" xml:base="https://hanqing-shi.github.io/blog/2023/linear-algebra-note-1/"><![CDATA[<h1 id="least-sqaures">Least Sqaures</h1> <p>This is the 3rd note based on <em><a href="https://math.mit.edu/~gs/learningfromdata/">Linear Algebra And Learning From Data</a></em> by <em>Gilbert Strang</em>.</p> <hr/> <h3 id="the-big-picture">The Big Picture</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Bigpic-480.webp 480w,/assets/img/Bigpic-800.webp 800w,/assets/img/Bigpic-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Bigpic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="least-squares">Least Squares</h3> <p>Many applications lead to <strong>unsolvable</strong> linear equations $Ax=b$. The least squares method chooses $\hat x$ to make $||b-A\hat x||^2$ as <strong>small</strong> as possible. Which is $(Ax-b)^T(Ax-b)$. Minimizing erorr means its derivatives are zero which leads to <strong>normal eqautions</strong> $A^TA\hat x=A^Tb$.</p> <h3 id="four-ways-to-solve">Four ways to solve</h3> <ol> <li>The SVD of A leads to its <strong>pseudoinverse</strong> $A^+$. Then $\hat x = A^+b$ :one short formula (This note)</li> <li>$A^TA\hat x=A^Tb$ can be solved directly when A has <strong>independent columns</strong></li> <li>The Gram-Schmidt idea produces <strong>orthogonal columns</strong> in Q which is $A=QR$.</li> <li> <table> <tbody> <tr> <td><strong>Minimize</strong> $</td> <td> </td> <td>b-Ax</td> <td> </td> <td>^2 + \delta^2</td> <td> </td> <td>x</td> <td> </td> <td>^2$</td> </tr> </tbody> </table> </li> </ol> <h4 id="a-is-pseudoinverse-of-a">$A^+$ is Pseudoinverse of A</h4> <p>Recall from four subspaces</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/foursubspaces-480.webp 480w,/assets/img/foursubspaces-800.webp 800w,/assets/img/foursubspaces-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/foursubspaces.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>When A multiplies a vector x in its rowspace (otherwise Ax = 0), this produces Ax in the column space. If A is invertible, then $A^+ =A^{-1}$, and we have $A^+Ax=x$ <strong>exactly when x is in the row space</strong>. And $AA^+b=b$ when b is in the column space.<br/> We conclude:</p> <ul> <li>$A$: Row space to column space</li> <li>$A^+$: Column space to row space</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/A-480.webp 480w,/assets/img/A-800.webp 800w,/assets/img/A-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/A.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="rules-to-get-pseudoinverse">Rules to get pseudoinverse</h4> <ol> <li>If A has independent columns, then $A^+=(A^TA)^{-1}A^T$ and $A^+A=I$</li> <li>If A has independent rows, then $A^+=A^T(AA^T)^{-1}$ and $AA^+=I$</li> <li>A diagonal matrix $\sum_{m<em>n}$ has $\sum ^{+} _{n</em>m}$</li> </ol> <p>for <strong>all matrices</strong>: $A=U\sum V^T$ (SVD) , $A^+=V\sum^+U^T$</p> <h5 id="the-least-squares-solution-to-axb-is-x--ab">The Least Squares Solution to $Ax=b$ is $x^+ = A^+B$</h5> <p>properties:</p> <ul> <li> <table> <tbody> <tr> <td>$x^+=A^+b$ makes $</td> <td> </td> <td>b-Ax</td> <td> </td> <td>^2$ as small as possible.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>If another $\hat x$ achieves then $</td> <td> </td> <td>x^+</td> <td> </td> <td>\le</td> <td> </td> <td>\hat x</td> <td> </td> <td>$ (Minimum norm)</td> </tr> </tbody> </table> </li> </ul>]]></content><author><name></name></author><category term="notes"/><category term="math"/><summary type="html"><![CDATA[Least squares and the ways to solve it.]]></summary></entry><entry><title type="html">Covariance</title><link href="https://hanqing-shi.github.io/blog/2023/statistics-note-3/" rel="alternate" type="text/html" title="Covariance"/><published>2023-01-26T15:12:00+00:00</published><updated>2023-01-26T15:12:00+00:00</updated><id>https://hanqing-shi.github.io/blog/2023/statistics-note-3</id><content type="html" xml:base="https://hanqing-shi.github.io/blog/2023/statistics-note-3/"><![CDATA[<h1 id="statistics">Statistics</h1> <p>This is the 2nd note based on <em><a href="https://math.mit.edu/~gs/learningfromdata/">Linear Algebra And Learning From Data</a></em> by <em>Gilbert Strang</em>.</p> <hr/> <h3 id="1covariance-matrices-and-joint-probabilities">1.Covariance Matrices and Joint Probabilities</h3> <p>Linear algebra enters When we run M different experiments. For example, we measure <strong>age and height and weight</strong> to get a vector $m=(m_1,m_2,m_3)$ in which $M=3$.</p> <p>A matrix becomes involved when we look at variances. Each experiment (dimension) will have a sample variance or expected variance. If we measure age and height and weight for children, the results will be strongly <strong>correlated</strong>. The connection of variables is denoted by <strong>covariance</strong>.</p> \[\sigma_{12}=E[(m_1-\mu_1)(m_2-\mu_2)]\] <p>also:</p> \[\sigma_{xy}=E[(X-\bar X)(Y-\bar Y)]\] <p>Joint probability $p_{ij}$ : experiment1 produces i and experiment2 produces j.</p> \[\sigma_{12}=\sum_{i}\sum_{j}p_{ij}(x_i-m_1)(y_j-m_2)\] <p>Probability Matrix:</p> \[\begin{pmatrix} p_{11} &amp; p_{12} \\ p_{21} &amp; p_{22} \\ \end{pmatrix}\] <p>For independent trials we have $\sigma_{ij}(i\ne j) = 0$ , because $p_{ij}=p_ip_j$</p> <p>Covariance Matrix (By definition):</p> \[V = \sum \sum p_{ij} \begin{pmatrix} (x_i-m_1)^2 &amp; (x_i-m_1)(y_j-m_2) \\ (x_i-m_1)(y_j-m_2) &amp; (y_j-m_2)^2 \\ \end{pmatrix}\] <p>On the diagonal, we are getting the ordinary variances $\sigma_1^2, \sigma_2^2$ :</p> \[V_{11} = \sum\sum p_{ij} (x_i-m_1)^2=\sum p_i (x_i-m_1)^2=\sigma_{1}^2\] <p>We can write V in vector multiplication form:</p> \[\begin{bmatrix} (x_i-m_1)^2 &amp; (x_i-m_1)(y_j-m_2) \\ (x_i-m_1)(y_j-m_2) &amp; (y_j-m_2)^2 \\ \end{bmatrix}\] \[=\begin{bmatrix} x_i-m_1 \\ y_j-m_2 \\ \end{bmatrix} \begin{bmatrix} x_i-m_1 &amp; y_j-m_2 \\ \end{bmatrix}\] \[=(X-\bar X)(X-\bar X)^T\] <p>We know that V is <strong>sum of rank 1 matrices</strong> and <strong>symmetric</strong> matrix (Positive Semidefinite also).</p> \[V=\sum p_{ij}(X-\bar X)(X-\bar X)^T\] <p>for continous r.v.:</p> \[V=\int p_{ij}(X-\bar X)(X-\bar X)^T\] <h4 id="the-covariance-matrix-for-zax">The covariance matrix for $Z=AX$</h4> \[Z=AX, V_z=AV_XA^T\] <p>Example:</p> \[A=\begin{bmatrix} 1 &amp; 1 \ \end{bmatrix}, Z=X+Y\] <p>The variance of $Z$, denoted as $\sigma_z^2$, can be computed as:</p> \[\begin{bmatrix}1&amp;1 \end{bmatrix}\begin{bmatrix}\sigma_x^2&amp;\sigma_{xy}\\\sigma_{xy}&amp; \sigma_y^2 \end{bmatrix}\begin{bmatrix}1\\1 \end{bmatrix}=\sigma_x^2+\sigma_y^2+2\sigma_{xy}\] <h4 id="the-correlation-rho">The Correlation $\rho$</h4> <p>The correlation coefficient between $X$ and $Y$ is denoted as $\rho_{xy}$, and it satisfies the following inequality:</p> \[-1\le \rho _{xy}=\frac{\sigma_{xy}}{\sigma_x\sigma_y} \le1\] <p>We can standardize $X$ and $Y$ by dividing them with their respective standard deviations, i.e., $X=\frac{x}{\sigma_x}$ and $Y=\frac{y}{\sigma_y}$, so that they have zero mean and unit variance. The correlation coefficient $\rho_{xy}$ remains the same after standardization.</p>]]></content><author><name></name></author><category term="notes"/><category term="math"/><summary type="html"><![CDATA[A simple interpretation about covariance.]]></summary></entry><entry><title type="html">Statistical Inference</title><link href="https://hanqing-shi.github.io/blog/2023/statistics-note-2/" rel="alternate" type="text/html" title="Statistical Inference"/><published>2023-01-10T15:12:00+00:00</published><updated>2023-01-10T15:12:00+00:00</updated><id>https://hanqing-shi.github.io/blog/2023/statistics-note-2</id><content type="html" xml:base="https://hanqing-shi.github.io/blog/2023/statistics-note-2/"><![CDATA[<h1 id="statistics-note">Statistics Note</h1> <p>This is a note based on <a href="https://ocw.mit.edu/courses/18-650-statistics-for-applications-fall-2016/">MIT 18.650</a>.</p> <hr/> <h3 id="1trinity-of-statistical-inference">1.Trinity of statistical inference</h3> <ol> <li>Estimation</li> <li>Confidence intervals</li> <li>Hypothesis testing</li> </ol> <h3 id="2statistical-model">2.Statistical model</h3> \[(E,\rm (I\!P_\theta)_{\theta \in \Theta})\] <p>E: Sample space</p> <p>$(P_\theta)_{\theta \in \Theta}$ is a family of probability</p> <p>$\Theta$:parameter set</p> <h3 id="3estimation">3.Estimation</h3> <h4 id="parameter-estimation">Parameter Estimation</h4> <p>Statistic: Any measurable <strong>function</strong> of the sample, e.g.,<br/> $\bar X_n, maxX_i, X_1 + log(1 + |X_n|)$, sample variance etc.</p> <p>Estimator of $\theta$: Any statistic whose expression does not depend on $\theta$. (<strong>r.v.</strong>)</p> \[\hat{\theta}_n\to\theta\] <p>Asymptotically normal:</p> \[sqrt{n}(\hat{\theta}_n-\theta)\to N(0,\sigma^2)\] <h4 id="bias-of-an-estimator">Bias of an estimator</h4> <p>\(bias(\hat{\theta}_n)=E[\hat{\theta}_n]-\theta\) If bias = 0, we say $\hat{\theta}_n$ is <strong>unbiased</strong>.</p> <h4 id="quadratic-risk">Quadratic risk</h4> <p>Idea: We want estimators to have <strong>low bias</strong> and <strong>low variance</strong> at the same time.<br/> Quadratic risk:</p> \[R(\hat{\theta}_n)=E[(\hat{\theta}_n-\theta)^2]=Var(\hat{\theta}_n)+bias(\hat{\theta}_n)^2\] <h3 id="4-confidence-intervals">4. Confidence intervals</h3> <p>Let $(E,\rm (I!P_\theta)_{\theta \in \Theta})$ be a statistical model based on observations $X_1,\cdots,X_n$. Let $\alpha\in(0,1)$</p> <h4 id="confidence-interval-ci-of-level-1-alpha-for-theta">Confidence interval (C.I.) of level 1-$\alpha$ for $\theta$:</h4> <p>Any <strong>random</strong> (depending on $X_1,\cdots,X_n$. Let $\alpha\in(0,1)$) interval $I$ whose boundaries do not depend on $\theta$ and such that</p> \[\rm I\!P_\theta[\theta \in I]\ge 1-\alpha,\forall \theta \in \Theta\] <p>We bulid an <strong>error bar</strong> around estimator. It is also worth noticing that $p$ ( or $\theta$ ) is a <strong>deterministic</strong> number, whereas $\bar R_n $ is a <strong>r.v.</strong> , $I$ is produced based on $\bar R_n $.</p> <p>example (for Bernoulli, $R_i \in Ber(p)$):</p> \[\sqrt{n}\frac{\bar{R_n}-p}{\sqrt{p(1-p)}}\stackrel{(d)}{\longrightarrow} N(0,1)\] <p>which is:</p> \[\sqrt{n}({\bar{R_n}-p)}\stackrel{(d)}{\longrightarrow} N(0,\sigma^2)\] <p>By CDF:</p> \[\rm I\!P[|\bar R_n-p|\ge x] \approx 2(1-\phi(\frac{x\sqrt{n}}{\sqrt{p(1-p)}}))=\alpha\] <p>We can solve for x (use the notation of <strong>Quantile</strong>):</p> \[\frac{x\sqrt{n}}{\sqrt{p(1-p)}}=\phi^{-1}(1-\frac{\alpha}{2})=q_{\frac{\alpha}{2}}\] <p>So we can get the interval:</p> \[\bar R_n \in[\bar R_n-\frac{q_{\frac{\alpha}{2}}\sqrt{p(1-p)}}{\sqrt{n}},\bar R_n+\frac{q_{\frac{\alpha}{2}}\sqrt{p(1-p)}}{\sqrt{n}}]\] <p>But this is <strong>not</strong> a confidence interval (depends on p).</p> <h5 id="solution-1-conservative-bound">Solution 1: Conservative Bound</h5> \[p(1-p) \le \frac{1}{4}\] <h5 id="solution-2-solving-the-quadratic-equation-for-p">Solution 2: Solving the (quadratic) equation for p</h5> <p>We have the system of two inequalities in p:</p> \[\bar R_n-\frac{q_{\frac{\alpha}{2}}\sqrt{p(1-p)}}{\sqrt{n}}\le p \le \bar R_n+\frac{q_{\frac{\alpha}{2}}\sqrt{p(1-p)}}{\sqrt{n}}\] <p>Each is a quadratic inequality in p of the form：</p> \[(p-\bar R_n)^2 \le \frac {q_\frac{\alpha}{2}^2 p(1-p)} {n}\] <p>solve $p_1,p_2$ to get the interval $[p_1,p_2]$</p> <h5 id="solution-3-plug-in">Solution 3: Plug-in</h5> <p>(by slutsky)</p> <h4 id="the-delta-method">The Delta Method</h4> <p>Let $Z_n$ be a sequence of r.v.that satisfies</p> \[\sqrt{n}(Z_n-\theta)\xrightarrow[n\to \infty]{(d)} N(0,\sigma^2)\] <p>Let $g:\rm I!R \to \rm I!R$ be continuously differentiable at the point $\theta$<br/> Then</p> \[\sqrt{n}(g(Z_n)-g(\theta))\xrightarrow[n\to \infty]{(d)} N(0,g^{'}(\theta)^2\sigma^2)\] <h3 id="5hypothesis-testing">5.Hypothesis testing</h3> <h4 id="statistical-formulation">Statistical formulation</h4> <p>Consider a sample of $X_1,X_2,\cdots ,X_n$ of i.i.d. r.v. and a statistical model $(E,\rm (I!P_\theta)_{\theta \in \Theta})$. Let $\Theta _0$ and $\Theta _1$ be disjoint subsets of $\Theta$.</p> <p>Consider the two hypotheses:</p> \[H_0 : \theta \in \Theta _0\] \[H_1: \theta \in \Theta _1\] <p>$H_0$ is the <strong>null hypothesis</strong>, $H_1$ is the <strong>alternative hypothesis</strong>.</p> <h4 id="asymmetry-in-the-hypothesis">Asymmetry in the hypothesis</h4> <p>$H_0$ and $ H_1$ <strong>do not</strong> play a symmetric role: the data is is only used to try to disprove $H_0$.<br/> In particular lack of evidence, does not mean that $H_0$ is true.<br/> A test is a statistic $\psi \in${ $0,1$ } such that:<br/> If $\psi =0$ ,$H_0$ is <strong>not rejected</strong>.<br/> If $\psi =1$ ,$H_0$ is <strong>rejected</strong>.</p> <h4 id="errors">Errors</h4> <p>Type 1 error of a test (rejecting $H_0$ when it is actually true) : $\alpha _\psi$</p> <p>Type 2 error of a test (not rejecting $H_0$ although $H_1$ is actually true)</p> <h4 id="level">Level</h4> <p>A test $\psi$ has level $\alpha$ (error 1) if</p> \[\alpha _\psi (\theta) \le \alpha\] <h4 id="one-sided-vs-two-sided-tests">One-sided vs two-sided tests</h4> <p>If $H_1：\theta \ne \theta _0$ : <strong>two-sided test</strong><br/> If $H_1：\theta \gt \theta _0$ or $H_1：\theta \lt \theta _0$: <strong>one-sided test</strong></p> <h4 id="examplebernoulli">Example(Bernoulli)</h4> <p>\(H_0: p\le0.33,\ H_1:p\ge0.33\) Reject if $\hat p = \bar X_n \gt \lambda$ (to be chosen later)</p> \[max\ _{p\le0.33}\rm I\!P_\theta[\bar X_n \gt \lambda]\to \alpha\ (Error1)\] <p>By normalization:</p> \[\rm I\!P_\theta[\frac{\sqrt n(\bar X_n-p)}{\sqrt{p(1-p)}} \gt \frac{\sqrt n(\lambda-p)}{\sqrt{p(1-p)}}]\to \alpha\] <p>So the RHS: $q_\alpha$ or $q_{1-\alpha}$(less than)</p> <h4 id="p-value">p-value</h4> <p>Definition: the <strong>smallest</strong> (asymptotic) level $\alpha$ at which $\psi_\alpha$ rejects $H_0$<br/> Golden rule:<br/> $\alpha \ge p$, $H_0$ is rejected.$H_0$ is more likely to be rejected as $\alpha$ increases.</p>]]></content><author><name></name></author><category term="notes"/><category term="math"/><summary type="html"><![CDATA[Key concepts in statistical inference.]]></summary></entry><entry><title type="html">Statistics 101</title><link href="https://hanqing-shi.github.io/blog/2023/statistics-note-1/" rel="alternate" type="text/html" title="Statistics 101"/><published>2023-01-09T15:12:00+00:00</published><updated>2023-01-09T15:12:00+00:00</updated><id>https://hanqing-shi.github.io/blog/2023/statistics-note-1</id><content type="html" xml:base="https://hanqing-shi.github.io/blog/2023/statistics-note-1/"><![CDATA[<h1 id="statistics-note">Statistics Note</h1> <p>This is the 1st note based on <em><a href="https://math.mit.edu/~gs/learningfromdata/">Linear Algebra And Learning From Data</a></em> by <em>Gilbert Strang</em>.</p> <hr/> <h3 id="1big-picture">1.Big picture</h3> <center><img src="/assets/img/statistics1.png"/></center> <p>When an ouput is predicted, we need its probability. When that output is measured,we need its statistics.</p> <h3 id="2meanvariance">2.Mean,Variance</h3> <p><strong>Sample mean</strong> :</p> \[m=\mu=\frac{1}{N}(x_{1}+x_{2}+\cdots+x_{N})\] <p><strong>Expected Value</strong> :</p> \[m=E[x]=p_{1}x_{1}+p_{2}x_{2}+\cdots+p_{n}x_{n}\] <p>Law of Large Numbers: $\mu\to E[x]$ , as N increases.</p> <p><strong>Sample Variance</strong>:</p> \[S^{2}=\frac{1}{N-1}[(x_{1}-m)^2+(x_{2}-m)^2+\cdots +(x_{N}-m)^2]\] <p><strong>Variance</strong>:</p> \[\sigma^2=E[(x-E[x])^2]=p_1(x_1-m)^2+p_2(x_2-m)^2+\cdots +p_n(x_n-m)^2\] <p>The variance $\sigma ^2$ measures the expected distance(squared) from the expected mean E[x]. <br/> The sample variance measures actual distance(squared) form the actual sample mean $\mu$.</p> <p><strong>Notice!</strong> : the denominator in sample variance is <strong>N-1</strong>,so that $S^2$ is unbiased estimate of $\sigma^2$. (Bessel’s Correction)</p> <p>Proof：</p> \[\begin{align*} E[S^2]&amp;=E[\frac{1}{N-1}\sum(x_i-m)^2]=\frac{1}{N-1}E[\sum x_i^2-2\sum x_im+\sum m^2] \\ &amp;=\frac{1}{N-1}E[\sum x_i^2-Nm^2]=\frac{1}{N-1}[E[\sum x_i^2]-E[Nm^2]] \end{align*}\] <p>Notice that:</p> \[\begin{align*} E[\sum x_i^2] &amp;= \sum E[x_i^2] \\ &amp;= \sum (var(x_i)+E[x_i]^2) \\ &amp;= N(\sigma^2 +\mu^2) \end{align*}\] <p>similarly:</p> \[\begin{align*} E[Nm^2] &amp;= N E[m^2] \\ &amp;= N(var(m)+E[m]^2) \\ &amp;= N(\frac{1}{N}\sigma^2+\mu^2) \end{align*}\] <p>Therefore:</p> \[E[S^2]=\frac{1}{N-1}[(N-1)\sigma^2]=\sigma^2\] <h3 id="3probability-distributions">3.Probability Distributions</h3> <table> <thead> <tr> <th style="text-align: center">Distribution</th> <th style="text-align: center">One-word Description</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Binomial</td> <td style="text-align: center">Tossing a coin $n$ times</td> </tr> <tr> <td style="text-align: center">Poisson</td> <td style="text-align: center">Rare events</td> </tr> <tr> <td style="text-align: center">Exponential</td> <td style="text-align: center">Forgetting the past</td> </tr> <tr> <td style="text-align: center">Gaussian</td> <td style="text-align: center">Averages of many tries</td> </tr> <tr> <td style="text-align: center">Log-normal</td> <td style="text-align: center">Logarithm has normal distribution</td> </tr> <tr> <td style="text-align: center">Chi-squared</td> <td style="text-align: center">Distance squared in $n$ dimensions</td> </tr> <tr> <td style="text-align: center">Multivariate Gaussian</td> <td style="text-align: center">Probabilities for a vector</td> </tr> </tbody> </table> <h4 id="binomial"><strong>Binomial</strong></h4> \[\mu = np,\sigma^2=np(1-p)\] <h4 id="poisson"><strong>Poisson</strong></h4> \[p \to 0, \quad n \to \infty, \quad np = \lambda \\\] <p>binomial:</p> \[p_{0,n} = (1-p)^n = \left(1 - \frac{\lambda}{n}\right)^n \to e^{-\lambda} \\ p_{1,n} = np(1-p)^{n-1} = \frac{\lambda}{1-p}\left(1 - \frac{\lambda}{n}\right)^n \to \lambda e^{-\lambda}\] <h4 id="poisson-probability"><strong>Poisson probability</strong></h4> \[P_k=\frac{\lambda ^k}{k!}e^{-\lambda} \\ \mu=\lambda\\ \sigma^2=\lambda\] <h4 id="exponential-distribution"><strong>Exponential distribution</strong></h4> <p>It describes the <strong>waiting time</strong> in a poisson process.(continous,memoryless)</p> \[p(x)=\lambda e^{-\lambda x}(x\ge 0) ,F(t)=1-e^{-\lambda t}\] \[\mu=\frac{1}{\lambda},\sigma^2=\frac{1}{\lambda ^2}\] <h4 id="chi-squared-distribution"><strong>Chi-squared Distribution</strong></h4> \[\chi ^2_{n}=\sum x_{i}^2\] <p>where $x_{i}$ are independent standard normal r.v.</p> <p>The Gamma function</p> \[\Gamma(n)=(n-1)!\] <h5 id="typical-use-">Typical use :</h5> <p>$S^2$ is a sum of squares with $n-1$ degree freedom. It has the probability distribution $p_{n-1}$ for $\chi_{n-1} ^2$<br/> Example:</p> \[for\ n=2,S^2=\frac{1}{2}(x_1-x_2)^2\]]]></content><author><name></name></author><category term="notes"/><category term="math"/><summary type="html"><![CDATA[An illustration of the big picture as well as common concepts and distributions in statistics.]]></summary></entry></feed>